---
title: "Final Project"
author: "The Steve Henle, Hazel John, and Jim Schlough Group"
date: "April 28, 2016"
output:
  html_document:
    toc: true
    toc_depth: 5
    fig_caption: true
---
<script language="javascript"> 
    function toggle(num) {
      var ele = document.getElementById("toggleText" + num);
      var text = document.getElementById("displayText" + num);
      if(ele.style.display == "block") {
        ele.style.display = "none";
        text.innerHTML = "show";
      }
      else {
        ele.style.display = "block";
        text.innerHTML = "hide";
      }
   } 
  </script>
  

<a id="top"></a>

```{r echo = FALSE, message=FALSE, warning=FALSE}
require(knitr)
opts_chunk$set(cache = TRUE, message = FALSE)

# Load required libraries
if (!require(tidytext))
  install.packages("tidytext")
if (!require(wordcloud))
  install.packages("wordcloud")
if(!require(caret))
  install.packages("caret")
if(!require(maps))
  install.packages("maps")
library(ggplot2)
library(gridExtra)
library(readr)
library(lubridate)
library(dplyr)
library(tidyr)
library(stringr)
library(reshape2)
library(maps)

theme_set(theme_bw(base_size = 16))
```

# A Study of the Data Provided by the Donors Choose Organization 
by
Steve Henle<br>
Hazel John<br>
Jim Schlough<br>

## Overview and Motivation
Funding the contemporary K-12 classroom is greatly challenging and many teachers believe that resources provided are insufficient in meeting the most basic objectives. Nontraditional support is playing an increasingly important part in supporting the modern classroom. 

The Donors Choose web platform provides a mechanism of providing support to teachers by benefactors. A potential donor may read an appeal written by a teacher to fulfill a specific classroom material need, and donates towards their funding goal. If the funding goal is met, these materials are sourced by fulfillment sources under the control of the Donors Choose organization, and sent directly to the school. 

Given the changes in public sentiments and commitments towards financial support of the community school over the past few decades, the ability to raise funds directly into the classroom might come to be considered a vital skill of the teacher in supporting classroom activities. 

When written appeals succeed by becoming fully funded, or fail by expiring, the data surrounding the appeal is gathered and made publicly available. By analyzing this data, it might be possible to better understand the factors correlated with success or failure of a written appeal. Some factors such as location, date and time, and poverty level are beyond the control of a teacher. Other factors, such as the written content of an appeal, or to a lesser extent, the credentials of the teacher, can be controlled. 

We wish to apply statistical analysis of the available data in an effort to go beyond axiomatic and aesthetic beliefs regarding what makes a more or less effective funding proposal. 

<a href="#top">back to top</a><br>

## Related Work
Jim's sister is a grade school art teacher who successfully funded a 3D printer for her classroom. On last report, the best expert for this printer is a female 4th grader.

Steve writes grant proposals, so it is interesting to understand what makes a good grant proposal by way of a data analysis.

Hazel works with her marketing department on content marketing, personalization etc. She in interested in understanding what motivates prospects to become customers. In this case we analyze factors that encourage prospects to turn into benefactors.

Stephen Colbert made the news in 2015 [by funding every teacher's proposal for the State of South Carolina](http://time.com/3850694/stephen-colbert-funds-every-teacher-requested-grant-in-south-carolina-schools/). This did rise the national profile of Donors Choose, not only helping all the teachers in South Carolina, but also giving much needed publicity to this cause.
 
<a href="#top">back to top</a><br>

## Initial Questions
We set forth to answer the following questions regarding the funding proposals for Donors Choose.

*What makes the difference between a proposal that is funded and one that expires?<br> 
*Which are the winning qualities? <br>
*Of the predicting qualities, are there any that are under the control of the writer?<br>
*Does the content of the written essay matter?<br>

<a href="#top">back to top</a><br>

## Data
We are using the data made publicly available on the Donors Choose website. This publicly available comes in the form of downloadable csv files, ranging in size from megabytes to gigabytes. The data chosen was limited to projects submitted in 2014, for the sake of this introductory exercise getting the greatest number of runs on our laptops. This also limited the projects examined to only those having a certain funding outcome, and supressed effects of the overall national economic climate.<br>
This data can be found at [here](http://data.donorschoose.org/open-data/overview/).<br>

The proposals are one element in the open data set published from the Donors Choose web site. An image of the schema showing the suggested recomposition can be found by clicking the link below, and can also be found at Donors Choose.
<br>
<a href="#top">back to top</a><br>
<a id="displayText" href="javascript:toggle(4);">Show or hide source code for the suggested recomposition schema</a>
  <div id="toggleText4" style="display: none">
  This is schema diagram appears in the Donors Choose web site to serve as a diagram for the recomposition of the csv files:
![](http://i0.wp.com/data.donorschoose.org/wp-content/uploads/2013/12/DonorsChoose-org-data-schema-04012011.png)
<a href="#top">back to top</a><br>
<a id="dataloader_in_r"></a>
<br>
  
  </div>

### Data Wrangling


#### Step 1: Data Download & Cleanup

Our first task was to download the data, clean it up and extract the data we needed. We decided to do the analysis with just the data from 2014, so the final task was the filter out unneeded data.

<a href="#top">back to top</a><br>

#### rm_old: Our First Data Wrangling Utility 

Our first plan was to use a combination of AWK and SED to do the necessary data cleanup, but time & date fields proved to be problematic. After an evening of steady efforts along those lines, a C++ data cleaning application was written as a stop gap measure. This C++ data cleaner provided a temporary means to separate the 2014 projects records from the csv file and get the rest of the team started with the data. The source code for the intermediate c++ application, was named rm_old. Toggle the link below to see the source.
<br>
<a href="#top">back to top</a><br>
<a id="displayText" href="javascript:toggle(1);">Show or hide source code for rm_old data cleaner</a>
  <div id="toggleText1" style="display: none">
  
The original source code of the stop gap c++ data cleaning application is below.
<br><a id="rm_old"></a> <br>
####c++ Source code for rm_old
```{r original cpp app, eval=FALSE, echo=TRUE}
//
//  main.cpp
//  rm_old
//
//  Created by Jim Schlough on 4/22/16.
//  Copyright © 2016 Jim Schlough. All rights reserved.
//

#include <iostream>
#include <fstream>    // for ifstream, ofstream

#include <string>
#include <ctime>
#include <cstdlib>
#include <stdio.h>    // for tmpnam, remove

// for time & date processing:
#include <sstream>
#include <locale>
#include <iomanip>

using namespace std;

int main(int argc, const char * argv[]) {

    // insert code here...
    if (argc< 3 )
    {
        std::cout << "Usage rm_old fileInName bottomCutOffDate topCutOffDate dateFieldIndex" << endl;
        std::cout << endl;
        std::cout << "   dateFieldIndex is ONE based" << endl;
    }
    
    char filebuf [L_tmpnam];
    ::strcpy(filebuf, argv[1]);
    
    std::string outFileName;

    int dateFieldIdx = 0;
    dateFieldIdx = std::atoi(argv[4])-2;
    
    
    // TODO: check for clean cutOffDateInput here
    int64_t bottomCutOffDateValue = 0L, topCutOffDateValue = 0L;
    bottomCutOffDateValue = std::atol(argv[2]);
    topCutOffDateValue = std::atol(argv[3]);
    
    // TODO: check for valid (positive integer) date field index (1 based) here
    

    std::ifstream inputFile (filebuf, std::ios::in);
    outFileName.append(filebuf);
    outFileName.erase( outFileName.find(".csv"),4);
    outFileName.append("_output.csv");
    
    std::ofstream outputFile (outFileName, std::ios::out);
    std::string line, submittedDateTimeStr, submittedDateStr;
    bool skipFirst = true;
    
    if (inputFile.is_open())
    {
        std::getline(inputFile, line);
        skipFirst = (line.find('\"') == std::string::npos); // first line is  header
        
        
        while( inputFile)
        {
            if (skipFirst)
            {
                skipFirst = false;
                outputFile << line << endl;
            }
            else std::getline(inputFile, line);
            
            if (line.length() < 2) continue;
            size_t numberCommas = std::count(line.begin(), line.end(), ',');
            if (numberCommas < 43 ||
                line.find("\"") == std::string::npos ) // skip the header line, which has no "
                continue;
            
            // find the position of the date in the 41st field
            int x = 0;
            //std::string::size_type
            int lastPos=0, startOfDatePos = 0, endOfDatePos = 0;
            int64_t dateIntValue = 0L;
            
            // TODO: make magical 39 to be dateFieldIndex in future refinement
            
            while (x<43   &&  inputFile.good() ) {
                lastPos = (int)line.find(',', lastPos+1);
                if (x== (dateFieldIdx)) // date we seek is in the 41st field
                {
                    startOfDatePos = lastPos+2;
                } else if (startOfDatePos != 0)
                {
                    endOfDatePos = (int)line.find(',', startOfDatePos)-1; // ", is end of field, so -1 for " part
                    break;
                }
                x++;
            }
            
            submittedDateTimeStr = line.substr(startOfDatePos, endOfDatePos-startOfDatePos );  ///19);
            
            // truncate the hours, minutes and seconds off of the date
            submittedDateStr = submittedDateTimeStr.substr(0, submittedDateTimeStr.length()-9 );
            while(submittedDateStr.find('-') != std::string::npos )
                submittedDateStr = submittedDateStr.erase( submittedDateStr.find('-'), 1);
            dateIntValue = std::atol(submittedDateStr.c_str());//, std::locale("en_US.utf-8"));
            
            if (dateIntValue <= bottomCutOffDateValue || dateIntValue >= topCutOffDateValue)
                continue; // skip to the next record if this one is too early or too late
            
            if (outputFile.is_open())
                outputFile << line << endl;
            else
                exit(EXIT_FAILURE);
        }
        inputFile.close();
        outputFile.close();
    }
    return 0;
}

```

</div>
<a href="#top">back to top</a><br>

### Data Wrangling in R

During analysis of the data files retrieved from the Donor's Choose website, we noticed that it contained string fields with special characters that caused errors when using the standard read utilities for comma delimited files. The first plan was to read files line by line and use "gsub" to removed unnecessary characters, but this turned out very slow. We then tried to run the UNIX utility "sed" within the "pipe" function, but the special characters like ctrl-M made that problematic. The final iteration still used "sed" by was run as a system call from R. Each file required slight variations in the search patterns so different <datatype>_clnup.sed scripts were created for each. 

Our final data wrangling consisted of the following steps:

* Data download from the website, 
* Cleanup via "sed" with a  system call
* Extraction of CSV as a dataframe
* Filtering out projects not posted in 2014
* Storing cleaned dataframes as RDS files

<a id="displayText" href="javascript:toggle(2);">Show or hide source code succeeding R data cleaner</a>
  <div id="toggleText2" style="display: none">
  
```{r eval=FALSE }
# Create function to write data frame to zipped rds file
# The dataframe is split into smaller files depending on size
writeToDisk <- function(df, path) {
  # get the size of the data frame
  filesz = object.size(df)
  
  # Figure out if it needs to be split, we try to 
  # split into sizes ~ 250MB (before compression)
  numsplits = filesz %/% (150*1024*1024)
  
  # Split into subsets and write to disk in RDS format
  # so that we can preserve attritubes including type
  if (numsplits > 1) {
    # Split the dataframe into "numsplits" subsets
    df_split <- split(df, ntile(df$`_projectid`, numsplits))
    
    cat("Writing", numsplits, "files with prefix", path, "\n")
    
    # Save data to separate rds files
    # Wrap loop inside invisible() since we are not interested in
    # the return values
    invisible(lapply(names(df_split), function(x) {
      
      write_rds(df_split[[x]], paste0(path, x, "of", numsplits, ".rds.gz"),
                compress = "gz")
    }))
  }
  else {
    
    cat("Writing 1 file with prefix", path, "\n")
    write_rds(df, paste0(path, ".rds.gz"), compress = "gz")
  }
}

# Create function that download file of type "kind", removes special 
# characters and loads the data
retrieveData <- function(kind, needs_cleanup) {
  
  # Create the download link
  url <- paste0("https://s3.amazonaws.com/open_data/csv/opendata_",
                  kind, ".zip")
     
  # Create the path to download the file to           
  zipname <- paste0("data/opendata_", kind, ".zip")
  
  # Create the filename
  filename <- paste0("opendata_", kind, ".csv")
  
  cat("Downloading from", url, "...")
  
  # Download the file
  download.file(url, zipname)
  
  # Donations, resources and essays data files needed cleanup with
  # special characters, escaped characters etc. creating read errors.
  # Data cleanup was done using sed as a system call after
  # realizing that using pipe() to run sed from R was slow.
  # NOTE: The sed script was created on MacOS and might not be portable.
  # Tried to run sed inside pipe - scan(pipe(sed_cmd), sep = ",") 
  # but had too many issues with needing to use multiple escaped characters
  # Also tried readlines() followed by gsub() but the performance was poor.
  if (needs_cleanup) {
    # cleanup is needed so unzip, run sed and then read in data
    
    # unzip the file
    unzip(zipname, filename)
    
    # Create a sed command to clean out special characters
    sed_cmd <- paste0("sed -i '' -f ", kind,
                      "_clnup.sed ", filename)
    
    cat("Running data cleanup for", filename, "...")
    
    # Run the sed command
    system(sed_cmd)
    
    cat("Loading", kind, "...")
    
    # Read in the data
    assign(kind, read_csv(filename), envir=globalenv())
    
    # Remove files
    unlink(zipname)
    unlink(filename)
  }
  else {
    cat("Loading", kind, "...")
    
    # cleanup is not needed, so read in data directly
    assign(kind, read_csv(unz(zipname, filename)), envir=globalenv())

    # Remove zip file
    unlink(zipname)
  }
}

# Create the list the type of data files we want to download
types_list = c("projects", "resources", "donations", "essays")
  
# Note which files need cleanup
needs_cleanup = c(FALSE, TRUE, TRUE, TRUE)

# Download files, remove special characters and load data
for (index in seq(1:4)) {
  retrieveData(types_list[index], needs_cleanup[index])
}

# Convert dates to "Date" format
projects <- projects %>%
  mutate(date_posted = as_date(date_posted),
         date_completed = as_date(date_completed),
         date_thank_you_packet_mailed =
           as_date(date_thank_you_packet_mailed),
         date_expiration = as_date(date_expiration))

donations <- donations %>%
  mutate(donation_timestamp = as_date(donation_timestamp))

# Filter out projects that were posted in 2014
projects <- projects %>% filter(year(date_posted) == 2014)

# Select resources, donations and essays associated with
# 
resources <- resources %>%
  semi_join(projects, by = "_projectid")
donations <- donations %>%
  semi_join(projects, by = "_projectid")
essays <- essays %>%
  semi_join(projects, by = "_projectid")

# Save filtered data to disk
writeToDisk(df=projects, path="data/opendata_2014_projects")
writeToDisk(df=resources, path="data/opendata_2014_resources")
writeToDisk(df=donations, path="data/opendata_2014_donations")
writeToDisk(df=essays, path="data/opendata_2014_essays")

# Let us clean all the variables so as to be able to start 
# with a clean slate
rm(projects, resources, donations, essays, 
   types_list, needs_cleanup, retrieveData, writeToDisk)

# Cleanup memory
gc()
```
</div>

<a href="#top">back to top</a><br>

#### Step 2: Data Upload from disk

A simple function was written to upload the cleaned data from RDS files stored on disk. Once the data has been put to disk locally, this function is all that's needed to load the data objects. All data exploration and analysis was run on the uploaded data.

<a id="displayText" href="javascript:toggle(3);">Show or hide source code for data load from disk</a>
  <div id="toggleText3" style="display: none">
  
```{r Data Upload from disk, message=FALSE, warning=FALSE}

# Create function to load data from the rds files containing the
# name "kind".
uploadData <- function(kind) {
  temp <- list.files(path = "./data", 
                    pattern = paste0(".*", kind, ".*rds.gz"),
                    full.names = TRUE)
  # Read in the data
  tables <- lapply(temp, read_rds)
  
  # Combine multiple (or single) dataframes into one and return
  return (bind_rows(tables))
}

# Read in the different data sets
projects <- uploadData("projects")
resources <- uploadData("resources")
donations <- uploadData("donations")
essays <- uploadData("essays")

```
</div>
<a href="#top">back to top</a><br>

## Exploratory Analysis
We began an exploration of the data to see what relationships might be discovered within it, to compare the completed and expired projects.

```{r Exploratory Analysis, echo=FALSE}
#add column to project df called percent_funded that tells the percent money given over asked for
projects <- projects %>% mutate(percent_funded = (total_donations / total_price_including_optional_support))
```

A critical first step was to determine how to analyze success. There were two possible metrics we came up with: a binary funded or not funded outcome, or a granualar (money donated / money requested). The granualar option would give us more options in trying to model the data, whereas the first option would require using logistic regression. To determine if the granular option was feasable we first had to look out the distribution of (money donated / money requested) accross the proposals.

here is a histogram and qqplot showing the distribution of donation to requested donation success ratio (money donated / money requested):

```{r donation to requested donation success ratio, echo=FALSE}
#test for normalacy of percent funded
projects %>% ggplot(aes(percent_funded *100)) + geom_histogram() + xlab("Percent Donations/Funding Requested") + ylab("Number of proposals")
  
#hist(projects$percent_funded)
qqnorm(projects$percent_funded)
print("Data is nowhere near normal, looks like logistic analysis of funded vs non-funded make much more sense.")

```

Next, we will look at different factors contained in the projects file to determine if they affect the likliehood of getting funded. For all of the following charts Percent Funded is the number of funded grants / total applications.

####Demographics
First looked at the available demographic factors.

```{r Demographics US Map, echo=FALSE}

#load us map data
all_states <- map_data("state")
#plot all states with ggplot
map <- projects %>% ggplot()
map <- map + geom_polygon( data=all_states, aes(x=long, y=lat, group = group),
                       colour="gray1", fill="light green" )
map <- map + geom_point(data=projects, 
                    aes(y=school_latitude, x=school_longitude, 
                        size =  percent_funded * 10^-7, 
                        # or: total_price_excluding_optional_support * 10^-3,
                        label = 'Price in $1K' ), 
                    color= "blue", alpha = 1/50) + scale_size();

map 
```
Perhaps the map shown above, with apologies to Alaska and Hawaii, is most useful in showing the popularity and ubiquity of the Donors Choose platform. This seed has certainly taken root in a broad cross section of the national professional teaching community. If the existing funding mechanisms of taxation and distribution through elected representatives were sufficient to the need, would this platform be as widely adopted?<br>

```{r poverty level graph, echo=FALSE}

#poverty level graph
poverty <- projects %>% group_by(poverty_level, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(poverty_level) %>% 
  mutate(totals = sum(total), percent = (total/totals)*100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(poverty_level, percent, fill = poverty_level)) + geom_bar(stat="identity")  + 
  theme(legend.position="none") + scale_fill_brewer() + 
  theme(axis.text.x=element_text(angle=90, size=8)) + ylab("Percent Funded") 
 

#state graph
state <- projects %>% group_by(school_state, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_state) %>% 
  mutate(totals = sum(total), percent = (total/totals)*100 ) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_state, percent)) + geom_bar(stat="identity", color = 'blue') +
  theme(legend.position="none") +
  theme(axis.text.x=element_text(angle=90, size=8)) + ylab("Percent Funded") 


#school urban vs rual
urban <- projects %>% group_by(school_metro, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_metro) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  filter(school_metro == "rural" | school_metro == "suburban" | school_metro == "urban") %>%
  ggplot(aes(school_metro, percent, fill = school_metro)) + geom_bar(stat="identity") +
  theme(legend.position="none") + scale_fill_brewer() + 
  theme(axis.text.x=element_text(angle=90, size=8)) + ylab("Percent Funded")



lay <- rbind(c(1,2),
             c(3,3))

grid.arrange(poverty, urban, state, layout_matrix = lay)
```

Although each aspect looked like it could have an effect, none really had a major effect of the funding outcome.

####Teacher Attributes
We next looked at different parameters about the requesting teacher.

```{r Teacher Attributes, echo=FALSE}

#teacher prefix, sort of a stand in for teacher gender
prefix <- projects %>% group_by(teacher_prefix, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(teacher_prefix) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(teacher_prefix == "Dr." | teacher_prefix == "Mr." | teacher_prefix == "Mrs." | teacher_prefix == "Ms.") %>%
  filter(funding_status == "completed") %>%
  ggplot(aes(teacher_prefix, percent, fill = teacher_prefix)) + geom_bar(stat="identity") + ylab("Percent Funded") + xlab("Teacher Prefix") + theme(axis.title.x=element_text(size=12)) + theme(legend.position="none") + scale_fill_brewer(palette = "Reds")

#teach for america teacher
america <- projects %>% group_by(teacher_teach_for_america, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(teacher_teach_for_america) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(teacher_teach_for_america, percent, fill = teacher_teach_for_america)) + geom_bar(stat="identity") + ylab("") + xlab("Teach for America") + theme(axis.title.x=element_text(size=12)) + scale_x_discrete(labels=c("No","Yes")) + scale_fill_brewer(palette = "Reds") + theme(legend.position="none")

#ny teaching fellow
ny <- projects %>% group_by(teacher_ny_teaching_fellow, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(teacher_ny_teaching_fellow) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(teacher_ny_teaching_fellow, percent, fill = teacher_ny_teaching_fellow)) +
  geom_bar(stat="identity") + ylab("") + xlab ("NY Teaching Fellow") +
  theme(axis.title.x=element_text(size=12)) + scale_x_discrete(labels=c("No","Yes")) +
  scale_fill_brewer(palette = "Reds") + theme(legend.position="none")

grid.arrange(prefix, america, ny, ncol = 3)
```

Here we can see several factors that appear to make a difference. However, The number of Drs., teach for america fellows, and NY teaching fellows is extremely small (<1% combined), and ultimately are unlikely to prove useful in predicting success for most applicants. Perhaps these teacher attributes might be more predictive of the likelihood of producing any Donors Choose submission at all, which is also predictive of success. You have to be in to win.

####School Attributes
We next looked at difference in the schools themselves. Thes factors include: were they a charter school, magnet school, New Leaders New Schools affiliate(NLNS), Knowledge is Power public charter (KIPP) school, Chart School Promise ready, and did they have school year round.

```{r School Attributes, echo=FALSE}
#charter school
charter <- projects %>% group_by(school_charter, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_charter) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_charter, percent)) + geom_bar(stat="identity") + ylab("Percent Funded") +
  xlab("Charter School") + theme(axis.title.x=element_text(size=12)) +
  scale_x_discrete(labels=c("No","Yes"))

#magnent school
magnet <- projects %>% group_by(school_magnet, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_magnet) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_magnet, percent)) + geom_bar(stat="identity") + ylab("") + xlab("Magnet School") +
  theme(axis.title.x=element_text(size=12)) + scale_x_discrete(labels=c("No","Yes"))

#school year round
round <- projects %>% group_by(school_year_round, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_year_round) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_year_round, percent)) + geom_bar(stat="identity") + ylab("") + 
  xlab("Year Round School") + theme(axis.title.x=element_text(size=12)) +
  scale_x_discrete(labels=c("No","Yes"))

#new Leaders New Schools affiliated 
nlns <- projects %>% group_by(school_nlns, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_nlns) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_nlns, percent)) + geom_bar(stat="identity") + ylab("Percent Funded") + xlab("NLNS")+
  theme(axis.title.x=element_text(size=12)) + scale_x_discrete(labels=c("No","Yes"))

#knowledge is power public charter school (KIPP)
kipp <- projects %>% group_by(school_kipp, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_kipp) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_kipp, percent)) + geom_bar(stat="identity") + ylab("") + xlab("KIPP") +
  theme(axis.title.x=element_text(size=12)) + scale_x_discrete(labels=c("No","Yes"))

#school charter ready promise
promise <- projects %>% group_by(school_charter_ready_promise, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(school_charter_ready_promise) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(school_charter_ready_promise, percent)) + geom_bar(stat="identity") + 
  ylab("Percent Funded")+
  xlab("Charter Promise Ready") + theme(axis.title.x=element_text(size=12)) +
  scale_x_discrete(labels=c("No","Yes"))

grid.arrange(charter, magnet, promise, nlns, kipp, round, ncol = 3)
```

Again we can se several small effects, but again all of these variables affect a very small percentage of schools.

####Classroom Attributes
We continued to look at different factors of the classroom. These include: the class subject, focus area, grade level, and number of students the proposal will reach.

```{r Classroom Attributes, echo=FALSE}
#primary focus subject
subject <- projects %>% group_by(primary_focus_subject, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(primary_focus_subject) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  filter(primary_focus_subject !='') %>%
  ggplot(aes(primary_focus_subject, percent)) + 
  geom_bar(stat="identity", color = 'green') + 
  theme(axis.text.x=element_text(angle=90, size=8)) + 
  ylab("Percent Funded") + theme(axis.title.x=element_text(size=12)) + xlab("Subject") +
  theme(legend.position="none")

#primary focus area
focus <- projects %>% group_by(primary_focus_area, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(primary_focus_area) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  filter(primary_focus_area !='') %>%
  ggplot(aes(primary_focus_area, percent, fill = primary_focus_area)) + geom_bar(stat="identity") + 
  theme(axis.text.x=element_text(angle=90, size=8)) + ylab("Percent Funded") +
  theme(axis.title.x=element_text(size=12)) + xlab ("Focus Area") +
  scale_fill_brewer(palette = "Greens") + theme(legend.position="none")

#grade level
grade <- projects %>% group_by(grade_level, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(grade_level) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  filter(grade_level !='') %>%
  ggplot(aes(grade_level, percent, fill = grade_level)) + geom_bar(stat="identity") + 
  theme(axis.text.x=element_text(angle=90, size=8))  + ylab("Percent Funded") + 
  theme(axis.title.x=element_text(size=12)) + xlab("Grade Level") +
  scale_fill_brewer(palette = "Greens") + theme(legend.position="none")

#students reached
reached <- projects %>% ggplot(aes(x = students_reached, y = percent_funded *100)) + geom_smooth() + ylab("Percent Funded") + xlab("# Students Reached") + theme(axis.title.x=element_text(size=12))

grid.arrange(subject, focus, grade, reached, ncol = 2)

```
From here we can see that focus area and grade level seem to have little affect. Certain subjects like music and team sports appear to positively correlate. The number of students reached shows an interesting trend, where it decreases with more students reached. Though initially this might not make sense, in the next section we look at the financial aspects of the proposal, which better explains this result.

#### Financial Attributes

```{r Financial Attributes,echo=FALSE }

#resource type
resource <- projects %>% group_by(resource_type, funding_status) %>% summarise(total= n()) %>% 
  group_by(resource_type) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  filter(resource_type != "") %>%
  ggplot(aes(resource_type, percent, fill = resource_type)) + geom_bar(stat="identity") + 
  theme(axis.text.x=element_text(angle=90, size=8)) + ylab("Percent Funded") +
  xlab("Resource Type") +
  scale_fill_brewer(palette = "Blues") + theme(legend.position="none")

#eligible for double your impact match
double <- projects %>% group_by(eligible_double_your_impact_match, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(eligible_double_your_impact_match) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(eligible_double_your_impact_match, percent, fill = eligible_double_your_impact_match)) +
  geom_bar(stat="identity") + 
  ylab("Percent Funded") + scale_x_discrete(labels=c("No","Yes")) + xlab("Double Match") +
  scale_fill_brewer(palette = "Blues") + theme(legend.position="none")

#eligible for almost home match
almost <- projects %>% group_by(eligible_almost_home_match, funding_status) %>% 
  summarise(total= n()) %>% 
  group_by(eligible_almost_home_match) %>% 
  mutate(totals = sum(total), percent = (total/totals) * 100) %>% 
  ungroup %>% 
  filter(funding_status == "completed") %>%
  ggplot(aes(eligible_almost_home_match, percent, fill = eligible_almost_home_match)) +
  geom_bar(stat="identity") + 
  ylab("Percent Funded") + scale_x_discrete(labels=c("No","Yes")) + xlab("Almost Home Match") +
  scale_fill_brewer(palette = "Blues") + theme(legend.position="none")

#date posted
date <- projects %>% ggplot(aes(x = date_posted, y = percent_funded * 100)) + geom_smooth() + ylab("Percent Funded") + xlab("Date Posted")

#total price excluding optional support
price <- projects %>% ggplot(aes(x = total_price_excluding_optional_support, y = percent_funded * 100)) + geom_smooth()  + ylab("Percent Funded") + xlab("Total Cost")

lay2 <- rbind(c(1,2,3),
             c(4,4),
             c(5,5))

grid.arrange(resource, double, almost, date, price, layout_matrix = lay2)

```

Here things get a little more interesting. It seems technology supplies are less likely, and books more likely to be funded. If you have matching funds coming in, you are more likely to be funded, although the almost home match is a little self fulfilling since it is given to people quite close to their goal. <br>
Date posted seems to show a seasonal effect, with the best odds right before school starts and the worst shortly after it starts. Then we see total cost, and keep in mind the Y-axis we see this has the largest effect we have seen so far, with a precipitous dropoff in the beginning before leveling off. As you can see, the error increases as cost increases, so we thought this might be due to few proposals requesting this much. So we then looked at the histogram to see the distribution of total costs of requests.

```{r histogram showing cost of all proposals, echo=FALSE}
#histogram showing cost of all proposals
projects %>% ggplot(aes(total_price_excluding_optional_support)) + geom_histogram() + ylab("# Proposals") + xlab("Total Cost")
```

From this it appears that the vast majority of proposals are requests under $1000, so we looked at the distribution and success rates below $1000.

```{r}

#histogram filtered to only show those propsals less than $100
hist_filtered <- projects %>% filter(total_price_excluding_optional_support <1000) %>%
  ggplot(aes(total_price_excluding_optional_support)) + geom_histogram() + 
  ylab("# Proposals") + xlab("Total Cost")

#Filtered total price vs sucess
cost_filtered <- projects %>% filter(total_price_excluding_optional_support <1000) %>% ggplot(aes(x = total_price_excluding_optional_support, y = percent_funded*100)) + geom_smooth()  + ylab("Percent Funded") +
xlab("Total Cost")

grid.arrange(hist_filtered, cost_filtered)
```

Here we can draw two important conclusions. First the vast majority of proposal request less than $1000, (and most are between $100-500). Second, In this range their is a strong negative correlation with success, a correlation that appears much strong than any other factor we have analyzed so far.
 

### Building a Prediction Model for Proposal Success

The first attempt to build a model was to use the glm method in the train function and select several variables from the earlier exploratory analysis that looked to have an effect. These variables included: amount of money asked for, school state, primary focus, primary subject, resource type, date posted. We used glm because our outcome is either funded or not funded and glm works well for logistic regression.

```{r message=FALSE, warning=FALSE, echo=FALSE}
#create column for binary 1 = funded, 0 = not funded regression
projects <- projects %>% mutate(funded = ifelse(funding_status == "completed", 1 , 0))

fit_selected_components <- train(funded ~ total_price_excluding_optional_support + primary_focus_area + school_charter + primary_focus_subject + students_reached + school_state + resource_type + date_posted,  data=projects, method="glm", family="binomial")

fit_selected_components

```

The RMSE from this method is not great, but it is a start so we tried to improve on it. First we filtered out some of the uncommon variables, that appeared to have an effect in the exploratory analysis. For example if the teacher was in teacher in teach for America, a New York teach fellow, was it a charter school, or other school types. All total these only filtered out few percent of the applications, and are factors the requesters can't change so they are not useful in building a prediction model. We then ran the training again.

```{r message=FALSE, warning=FALSE, echo=FALSE}

#fliter out less used variables
projects2 <- projects %>% filter(school_year_round == 'f'& school_magnet == 'f'& school_charter == 'f'& school_nlns == 'f'& school_kipp == 'f'& school_charter_ready_promise == 'f'& teacher_teach_for_america == 'f'& teacher_ny_teaching_fellow == 'f')

#run train again
fit_selected_components_filtered <- train(funded ~ total_price_excluding_optional_support + primary_focus_area + primary_focus_subject + students_reached + school_state + resource_type + date_posted,  data=projects2, method="glm", family="binomial")

fit_selected_components_filtered

```

Removing these proposals did show an effect in an improvement of RMSE, even though it is . The next approach was that perhaps our model was trying to fit to many parameters, so we pared it down to only include the factors that the exploratory analysis showed to have the greatest effect. These factors were: cost, date of posting, and resource type.

```{r message=FALSE, warning=FALSE, echo=FALSE}
#run train again using most selective variables
fit_more_selective_components <- train(funded ~ total_price_excluding_optional_support +  date_posted + resource_type,  data=projects2, method="glm", family="binomial")

fit_more_selective_components

```

This shows an even greater improvement in RMSE. In looking back at the data it was clear that the cost requested has the strongest effect. So we decided to run train again to see if that variable alone might improve our accurary (as shown by RMSE).


```{r message=FALSE, warning=FALSE, echo=FALSE}
fit_cost_only <- train(funded ~ total_price_excluding_optional_support,  data=projects2, method="glm", family="binomial")

fit_cost_only

```

So using all the data, except the contents of the essay, provided by the requesters it appears that the best model we can build is using the total cost as a predictor.

<a href="#top">back to top</a><br>

#### Text Analysis  

##### Word Selection Analysis

As part of the data available from the DonorsChoose website, we were able to download the full text of the teacher-written requests accompanying all classroom projects. The heart of each teacher’s classroom project request is their written request, and we wanted to use text mining to see if we could extract information from the essays to predict which projects get funded. The two questions we chose to answer were:

* Does the sentiments expressed in the essay help predict whether it gets funded completed or not?
* Are there specific words that appear in the essays for projects that are fully funded, that don't appear in the incompletely funded ones?

**Sentiment Analysis**

We started by tokenizing the essay words and then using the sentiment lexicon from "nrc" to assign sentiments to the words. We then computed the percentage occurrence for the sentiments in essays grouped by the funding status of the projects and compared the difference betweed "completed" and "expired" projects.
<a id="displayText" href="javascript:toggle(6);">Show or hide source code for sentiment analysis</a>
  <div id="toggleText6" style="display: none"> 
```{r message=FALSE, warning=FALSE}

# Tokenize the essay and remove stop words and include only
# all alphabetic words. All words are lower case, so there is
# no need to transform
essays_tokenized <- essays %>%
  select(`_projectid`, `_teacherid`, essay) %>%
  unnest_tokens(essay_words,essay)  %>%
  filter(!essay_words %in% stop_words$word &
           grepl("^[[:alpha:]]*$", essay_words))

# Get the sentiment lexicon from mrc
nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  select(word, sentiment)

# Assign sentiments to words
essays_sentiments <- essays_tokenized %>%
  left_join(nrc, by = c("essay_words" = "word"))

# Include the funding_status of the projects
essays_sentiments <- essays_sentiments %>%
  left_join(projects, by = "_projectid") %>%
  select(`_projectid`, funding_status, essay_words, sentiment)

# Count the sentiment occurrence percentage
essays_sentiment_freq <-
  essays_sentiments %>%
  group_by(funding_status, sentiment) %>%
  summarise(sentiment_freq = n()) %>%
  group_by(funding_status) %>%
  mutate(occurrence_pct = sentiment_freq*100/sum(sentiment_freq)) %>%
  ungroup()

```
</div>

```{r}

# Plot the sentiment frequency and seperate by funding status
essays_sentiment_freq %>%
  filter(funding_status != "reallocated") %>%
  ggplot(aes(x=sentiment, y = occurrence_pct, fill = sentiment)) +
  geom_bar(stat="identity") +
  facet_grid(~funding_status) +
  theme(text = element_text(size = 10),
        title = element_text(size = 12),
        legend.key.size = unit(0.5, "cm"),
        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  ggtitle('Sentiment Occurrence% in Funding Essays') +
  coord_flip()
```

We can see from the graph that both positive and negitive sentiments occur at an almost identical rate completed funded as well as expired projects. We can see the minor differences more clearly in the data frame.

```{r warning=FALSE, echo=FALSE}
temp <- essays_sentiment_freq %>% 
        filter(funding_status != "reallocated") %>% 
        select(sentiment, funding_status, occurrence_pct) %>%
        spread(funding_status, occurrence_pct)
print(temp)
rm(temp)
```

**Word Analysis**

We then analyzed the occurrence of specific words in the essays for insights. We started by looking at the top 10 words for both fully funded and expired projects.

<a id="displayText" href="javascript:toggle(7);">Show or hide source code for Word Analysis</a>
  <div id="toggleText7" style="display: none">
```{r message=FALSE, warning=FALSE, echo=FALSE}
# Compute word frequency in essays regardless of sentiment
# or funding_status
essays_word_freq <-
  essays_sentiments %>%
  group_by(essay_words) %>%
  summarise(completed_freq = sum(funding_status == "completed"),
            expired_freq = sum(funding_status == "expired"))

# Plot the top 10 words for both funding status
p1 <- essays_word_freq %>%
  top_n(n=10, wt=completed_freq) %>%
  ggplot(aes(x=reorder(essay_words, completed_freq),
             y = log10(completed_freq))) +
  geom_bar(stat="identity", fill = "blue") +
  ggtitle('Top 10 Words (completed)') +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 10)) +
  xlab("essay_words") +
  coord_flip()

p2 <- essays_word_freq %>%
  top_n(n=10, wt=expired_freq) %>%
  ggplot(aes(x=reorder(essay_words, expired_freq), 
             y = log10(expired_freq))) +
  geom_bar(stat="identity",  fill = "green") +
  ggtitle('Top 10 Words (expired)') +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 10),
        plot.title = element_text(size = 10)) +
  xlab("essay_words") +
  coord_flip()
```
</div>

```{r}
grid.arrange(p1, p2, nrow=1)
```

The top 10 words for both categories were almost identical. The only thing that stood out a little was that "excited" was in the top ten for completed funded projects versus "technology" for expired projects. This could be related to the fact that technology is more expensive and thus there is a lower chance of the project getting fully funded.

We removed the common words for both project categories for the top 10 words to focus of the rest of the words.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Students stands out for both "completed" and "expired" projects,
# So create word clouds without it, to see the rest better
essays_word_freq <- essays_word_freq %>%
  filter(!essay_words %in% c("students", "school", "teach", 
                             "learning", "classroom", "love",
                             "reading", "learn", "art"))
```

We then used wordclouds to compare the words that occur the most in both funding categories. We started with high frequency words in essays for projects that were fully funded.

```{r message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Fig 1: Wordcloud for \"completed\" project essays", fig.align='center'}

# Create word cloud for funded essays
wordcloud(essays_word_freq$essay_words, essays_word_freq$completed_freq,
          scale=c(3,.02), min.freq=15000, max.words=150,
          random.order=FALSE, rot.per=0.15, colors=brewer.pal(8, "Dark2"))
```

Then we compared it with the high frequency words in essays for projects that expired and weren't fully funded.

```{r, message=FALSE, warning=FALSE, echo=FALSE, fig.cap="Fig 2: Wordcloud for \"expired\" project essays", fig.align='center' }
# Create word cloud for expired essays
wordcloud(essays_word_freq$essay_words, essays_word_freq$expired_freq,
          scale=c(3,.02), min.freq=15000, max.words=150, 
          random.order=FALSE, rot.per=0.15, colors=brewer.pal(8, "Dark2"))

```

Looking at the wordcloud, one of the things that stood out were the higher relative prominence of "books" for funded projects versus "technology" for expired projects.

Further analysis was done to see if we could identify words that occur at a high frequency in essays associated with either "completed" and "expired" projects, but don't show up in the other. We extracted the sentiments for those words to see if there were particular sentiments that are related to those words.

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Find words that rank high for "completed" essays but are at
# least 75% points lower in "expired" essays and vice versa
# The words are ranked based off of percentage of occurrence 
# in completed and essays. Add sentiments for those words
words_unmatched <- essays_word_freq %>%
  mutate(pct_rank_completed = percent_rank(completed_freq),
         pct_rank_expired = percent_rank(expired_freq)) %>%
  mutate(pct_rank_diff = (pct_rank_completed-pct_rank_expired)) %>%
  filter(abs(pct_rank_completed-pct_rank_expired) > 0.75) %>%
  left_join(nrc, by = c("essay_words" = "word")) %>%
  arrange(wt=abs(pct_rank_diff))

success_standout_sentiments <- anti_join(words_unmatched %>%
                                           filter(pct_rank_diff > 0) %>%
                                           distinct(sentiment) %>%
                                           arrange(desc(pct_rank_diff)) %>%
                                           select(sentiment),
                                         words_unmatched %>%
                                           filter(pct_rank_diff < 0) %>%
                                           distinct(sentiment) %>%
                                           arrange(pct_rank_diff) %>%
                                           select(sentiment),
                                         by = "sentiment")

# Print out the sentiments for the words that appear often in
# essays for "completed" projects, but not in essays for 
# "expired" projects
print(success_standout_sentiments)

```

We were able to identify "surprise" and "anticipation" as sentiments that show up more in essays associated with "completed" projects as opposed to "expired" projects.

<a href="#top">back to top</a><br>

#### Essay Length Analysis:  Comparing the word count for completed vs expired projects
We took a look to see, does the length that an essay has in terms of word count matter?

<a id="displayText" href="javascript:toggle(9);">Show or hide source code for data load from disk</a>
  <div id="toggleText9" style="display: none">
```{r win loss word breakdowns,  message=FALSE, warning=FALSE}

# getting some mutually exclusive sets of winning vs losing words
# total word counts of essays by project
essay_total_word_sums <- essays %>%
  select(`_projectid`, essay) %>%
  unnest_tokens(essay_words,essay) %>%  
  group_by(`_projectid`)  %>%
  summarize(word_count= n())  

#word_count_summaries<- colnames(c("Description", "Value"))
word_count_summaries<-rownames(c("2014 project count",  "2014 completed project count", "2014 completed project word count mean", "2014 completed project word count std dev", "2014 expired project count","word count sums of the expired projects", "word sum mean for expired projects", "word count for expired projects of 2014"))

# 2014 project count:
project_count_2014<-count(projects)$n

# separate out the completed projects
completed_projects<- projects %>% filter(funding_status=="completed")

# 2014 completed project count:
completed_project_count_2014<-count(completed_projects)$n

# get the word count sums of the completed projects
completed_total_word_sums <- left_join(completed_projects, essay_total_word_sums, by="_projectid")

# average word sum for a completed project in 2014
completed_project_mean_word_count<-mean(completed_total_word_sums$word_count)

# standard deviation of word count for completed projects of 2014
completed_project_sd_word<-sd(completed_total_word_sums$word_count)

# separate out the expired projects
expired_projects<- projects %>% filter(funding_status=="expired")

# 2014 expired project count:
expired_project_count_2014<-count( expired_projects)$n

# get the word count sums of the expired projects
expd_total_word_sums <- left_join(expired_projects, essay_total_word_sums,  by="_projectid")

# number with NA word counts:
exp_na_word_count<-count( expd_total_word_sums %>% filter( is.na(word_count) ))$n

# get rid of the ones with NA word counts: 
expd_total_word_sums<- expd_total_word_sums %>% filter(! is.na(word_count) )

# 2014 expired project count having word counts:
expd_total_word_sums_count<-count( expd_total_word_sums)$n

# average word sum for a expired project in 2014
expd_total_word_sums_mean<-mean(expd_total_word_sums$word_count)

# standard deviation of word count for expired projects of 2014
expd_total_word_sums_sd<-sd(expd_total_word_sums$word_count)

word_count_summaries<-rbind(word_count_summaries, project_count_2014)
word_count_summaries<-rbind(word_count_summaries, completed_project_count_2014)
word_count_summaries<-rbind(word_count_summaries, completed_project_mean_word_count)
word_count_summaries<-rbind(word_count_summaries, completed_project_sd_word)
word_count_summaries<-rbind(word_count_summaries, expired_project_count_2014)
word_count_summaries<-rbind(word_count_summaries, expd_total_word_sums_count)
word_count_summaries<-rbind(word_count_summaries, expd_total_word_sums_mean)
word_count_summaries<-rbind(word_count_summaries, expd_total_word_sums_sd )
```
</div>

```{r}
kable(word_count_summaries, digits = 0, caption = "Essay Word Counts")

```

<a href="#top">back to top</a><br>

##### Essay word count comparison results 
Does essay word count matter?

Here we look at the number of words in essays, to see if there is any significant difference between the number of words in completed and expired essays.

```{r Essay word count comparison results, message=FALSE, warning=FALSE, echo=FALSE}
# Does essay word count matter?

sprintf("On average, completed essays had essay word counts that were %3.4f %s than expired ones", abs(mean(completed_total_word_sums$word_count) - mean(expd_total_word_sums$word_count)), ifelse(mean(completed_total_word_sums$word_count) > mean(expd_total_word_sums$word_count), "longer", "shorter" ))

```
The length of essay, in terms of word count, does not seem to matter much all by itself.

<a href="#top">back to top</a><br>

## Final Analysis
After examining the data, we were able to draw some conclusions. We take one last look here at our different predictors. But it also might be useful to loosen the notion of *predictor* from being a factor that is strictly causative, to one that could be either causitive or correlational. 

###The Titular Effect
The title of a teacher might provide an indicator of advanced education. A positive correlation was found where a teacher had both an advanced degree and a greater likelihood of fundedness.

###Aspects of Essays
We looked at sentiments evoked by essays, and also the length of the number of words within the essays.<br>

####Essary Word Selection Sentiments
Greater similarity than dissimilarity was found among word selection within the essarys. Words expressing excitement were found with with greater frequency among the essarys within funded projects.<br>
####Essary Word Counts
On average, essays were about 301 words long for both essays that were funded or not, so this did not really show any predictive effect. Perhaps the best use of this information is in the context of explaining the difficulty in preparing this part of the project by the teacher.<br>

###The Seasonal Effect
The time of the school year for 2014 seemed to have some effect, with the best correlation found with projects submitted during back to school shopping. This is in line with the time of year parents expect to spend on school supplies, within the traditions of American culture.<br>

###Factors Uncontrollable by Teachers
These involve factors regarding school and classroom properties and attributes.<br>

###Project Cost
Of all of the predictors, the total project cost was found to the be most significant. It is not really suprising that the most wins in endurance races are the ones having the shortest finish lines.<br>

###Conculusion
We wish to salute all teachers who accept the challenges of providing education to a world in extreme need. We hope that this may be of some services to those who seek to education, so that they can have the resources they need in pursuit of this noble goal.

<br>
###Recommendations for Donors Choose Projects
*All projects that are unsubmitted fail. The presence of a submission is perhaps the most important factor, and is unexamined here.
*Keep request size in terms of dollars as small as possible.
*Submit projects within the month prior to the beginning of the school year. This might be most important for school administrators, who should consider that many teachers need to work Summer jobs.


###Topics for Further Investigation
We would have liked to get the lines around Alaska and Hawaii.

There are thirteen years of data available on the donors choose website, and we only examined the year of 2014. We are in a much better to have broader look at the data over time, with the code we have above.
 

## References and Attributions

1. The javascript code used here to hide and show code blocks was found at:
(http://stackoverflow.com/questions/34784121/interactively-show-hide-code-r-markdown-knitr-report)[http://stackoverflow.com/questions/34784121/interactively-show-hide-code-r-markdown-knitr-report]

2. R packages used in this project:
*Names given are supposed authors, but apologies are offered if any are partially or misattributed. Many of these packages have dependencies on other packages not listed here.*
+(caret)[http://topepo.github.io/caret/index.html] Max Kuhn<br>
+(dplyr)[https://github.com/hadley/dplyr] Hadley Wickham<br>
+(ggplot2)[https://github.com/hadley/ggplot2] Hadley Wickham<br>
+(gridExtra)[https://cran.r-project.org/web/packages/gridExtra/index.html]<br>Baptiste Auguie, Anton Antonov <br>
+(lubridate)[https://cran.r-project.org/web/packages/lubridate/index.html]<br>Garrett Grolemund [aut], Vitalie Spinu, Hadley Wickham , Jan Lyttle, Imanuel Constigan, Jason Law [ctb], Doug Mitarotonda [ctb], Joseph Larmarange , Jonathan Boiser, Chel Hee Lee [ctb]<br>
+(maps)[https://cran.r-project.org/web/packages/maps/maps.pdf]<br>Original S code by Richard A. Becker and Allan R. Wilks.
R version by Ray Brownrigg.
Enhancements by Thomas P Minka and Alex Deckmyn.<br>
We tried to get a choropleth of a 50 state map working, and (this post)[https://www.google.com/?ion=1&espv=2#q=R+us+map+50+state] seemed promising. Sorry, Alaska and Hawaii.
<br>
+(readr)[https://cran.r-project.org/web/packages/readr/index.html]<br>Hadley Wickham, Romain Francois, R Core Team (Date time code adapted from R), RStudio <br>
+(reshape2)[https://cran.r-project.org/web/packages/reshape2/index.html]<br>Hadley Wickham<br>
+(stringr)[]<br><br>
+(tidyr)[]<br><br>
+(tidytext)[]<br><br>
+(wordcloud)[]<br><br>


